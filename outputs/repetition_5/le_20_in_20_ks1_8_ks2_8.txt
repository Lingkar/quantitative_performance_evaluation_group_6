2020-10-23 10:10:38 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-10-23 10:10:39 INFO  SparkContext:54 - Running Spark version 2.3.1
2020-10-23 10:10:39 INFO  SparkContext:54 - Submitted application: lenet5
2020-10-23 10:10:39 INFO  SecurityManager:54 - Changing view acls to: buijnstersjan
2020-10-23 10:10:39 INFO  SecurityManager:54 - Changing modify acls to: buijnstersjan
2020-10-23 10:10:39 INFO  SecurityManager:54 - Changing view acls groups to: 
2020-10-23 10:10:39 INFO  SecurityManager:54 - Changing modify acls groups to: 
2020-10-23 10:10:39 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(buijnstersjan); groups with view permissions: Set(); users  with modify permissions: Set(buijnstersjan); groups with modify permissions: Set()
2020-10-23 10:10:39 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 34343.
2020-10-23 10:10:39 INFO  SparkEnv:54 - Registering MapOutputTracker
2020-10-23 10:10:39 INFO  SparkEnv:54 - Registering BlockManagerMaster
2020-10-23 10:10:39 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-10-23 10:10:39 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up
2020-10-23 10:10:39 INFO  DiskBlockManager:54 - Created local directory at /tmp/blockmgr-e46e1932-496b-4886-8c00-1dbb90a9b6cf
2020-10-23 10:10:39 INFO  MemoryStore:54 - MemoryStore started with capacity 366.3 MB
2020-10-23 10:10:39 INFO  SparkEnv:54 - Registering OutputCommitCoordinator
2020-10-23 10:10:40 INFO  log:192 - Logging initialized @3370ms
2020-10-23 10:10:40 INFO  Server:346 - jetty-9.3.z-SNAPSHOT
2020-10-23 10:10:40 INFO  Server:414 - Started @3526ms
2020-10-23 10:10:40 INFO  AbstractConnector:278 - Started ServerConnector@424703e8{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-10-23 10:10:40 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4040.
2020-10-23 10:10:40 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2dcd13ed{/jobs,null,AVAILABLE,@Spark}
2020-10-23 10:10:40 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1b3e2566{/jobs/json,null,AVAILABLE,@Spark}
2020-10-23 10:10:40 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@141a66c7{/jobs/job,null,AVAILABLE,@Spark}
2020-10-23 10:10:40 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@a363516{/jobs/job/json,null,AVAILABLE,@Spark}
2020-10-23 10:10:40 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@60493423{/stages,null,AVAILABLE,@Spark}
2020-10-23 10:10:40 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@27a24ab3{/stages/json,null,AVAILABLE,@Spark}
2020-10-23 10:10:40 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@38009f96{/stages/stage,null,AVAILABLE,@Spark}
2020-10-23 10:10:40 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1fa67753{/stages/stage/json,null,AVAILABLE,@Spark}
2020-10-23 10:10:40 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@23e6b71e{/stages/pool,null,AVAILABLE,@Spark}
2020-10-23 10:10:40 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@c74b5dc{/stages/pool/json,null,AVAILABLE,@Spark}
2020-10-23 10:10:40 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@423cc1b{/storage,null,AVAILABLE,@Spark}
2020-10-23 10:10:40 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@28933f23{/storage/json,null,AVAILABLE,@Spark}
2020-10-23 10:10:40 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3edb3368{/storage/rdd,null,AVAILABLE,@Spark}
2020-10-23 10:10:40 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@17a8ab3a{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-10-23 10:10:40 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@df1fec0{/environment,null,AVAILABLE,@Spark}
2020-10-23 10:10:40 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3caab27a{/environment/json,null,AVAILABLE,@Spark}
2020-10-23 10:10:40 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5b8d86f4{/executors,null,AVAILABLE,@Spark}
2020-10-23 10:10:40 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@92e07c4{/executors/json,null,AVAILABLE,@Spark}
2020-10-23 10:10:40 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@babffdf{/executors/threadDump,null,AVAILABLE,@Spark}
2020-10-23 10:10:40 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@62fbb5b2{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-10-23 10:10:40 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5576353f{/static,null,AVAILABLE,@Spark}
2020-10-23 10:10:40 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@fcc124c{/,null,AVAILABLE,@Spark}
2020-10-23 10:10:40 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@739c8c5c{/api,null,AVAILABLE,@Spark}
2020-10-23 10:10:40 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@581e7b72{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-10-23 10:10:40 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@214c4e04{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-10-23 10:10:40 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://bigdl.europe-west4-a.c.massive-network-289210.internal:4040
2020-10-23 10:10:40 INFO  SparkContext:54 - Added JAR file:///home/am72ghiassi/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar at spark://bigdl.europe-west4-a.c.massive-network-289210.internal:34343/jars/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar with timestamp 1603447840605
2020-10-23 10:10:40 INFO  SparkContext:54 - Added file file:/home/am72ghiassi/bd/quantitative_performance_evaluation_group_6/lenet5.py at spark://bigdl.europe-west4-a.c.massive-network-289210.internal:34343/files/lenet5.py with timestamp 1603447840654
2020-10-23 10:10:40 INFO  Utils:54 - Copying /home/am72ghiassi/bd/quantitative_performance_evaluation_group_6/lenet5.py to /tmp/spark-2e346c5b-4921-4184-bc84-31b0037487a0/userFiles-4f3fecf2-c330-4da7-a4da-1deead3f053d/lenet5.py
2020-10-23 10:10:40 INFO  SparkContext:54 - Added file file:///home/am72ghiassi/bd/spark/lib/bigdl-0.11.0-python-api.zip at spark://bigdl.europe-west4-a.c.massive-network-289210.internal:34343/files/bigdl-0.11.0-python-api.zip with timestamp 1603447840665
2020-10-23 10:10:40 INFO  Utils:54 - Copying /home/am72ghiassi/bd/spark/lib/bigdl-0.11.0-python-api.zip to /tmp/spark-2e346c5b-4921-4184-bc84-31b0037487a0/userFiles-4f3fecf2-c330-4da7-a4da-1deead3f053d/bigdl-0.11.0-python-api.zip
2020-10-23 10:10:40 INFO  StandaloneAppClient$ClientEndpoint:54 - Connecting to master spark://10.164.0.2:7077...
2020-10-23 10:10:40 INFO  TransportClientFactory:267 - Successfully created connection to /10.164.0.2:7077 after 52 ms (0 ms spent in bootstraps)
2020-10-23 10:10:41 INFO  StandaloneSchedulerBackend:54 - Connected to Spark cluster with app ID app-20201023101041-0060
2020-10-23 10:10:41 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38787.
2020-10-23 10:10:41 INFO  NettyBlockTransferService:54 - Server created on bigdl.europe-west4-a.c.massive-network-289210.internal:38787
2020-10-23 10:10:41 INFO  StandaloneAppClient$ClientEndpoint:54 - Executor added: app-20201023101041-0060/0 on worker-20201022163533-10.164.0.3-37685 (10.164.0.3:37685) with 2 core(s)
2020-10-23 10:10:41 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-10-23 10:10:41 INFO  StandaloneSchedulerBackend:54 - Granted executor ID app-20201023101041-0060/0 on hostPort 10.164.0.3:37685 with 2 core(s), 1024.0 MB RAM
2020-10-23 10:10:41 INFO  StandaloneAppClient$ClientEndpoint:54 - Executor added: app-20201023101041-0060/1 on worker-20201022163611-10.164.0.4-33289 (10.164.0.4:33289) with 2 core(s)
2020-10-23 10:10:41 INFO  StandaloneSchedulerBackend:54 - Granted executor ID app-20201023101041-0060/1 on hostPort 10.164.0.4:33289 with 2 core(s), 1024.0 MB RAM
2020-10-23 10:10:41 INFO  StandaloneAppClient$ClientEndpoint:54 - Executor updated: app-20201023101041-0060/1 is now RUNNING
2020-10-23 10:10:41 INFO  StandaloneAppClient$ClientEndpoint:54 - Executor updated: app-20201023101041-0060/0 is now RUNNING
2020-10-23 10:10:41 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, bigdl.europe-west4-a.c.massive-network-289210.internal, 38787, None)
2020-10-23 10:10:41 INFO  BlockManagerMasterEndpoint:54 - Registering block manager bigdl.europe-west4-a.c.massive-network-289210.internal:38787 with 366.3 MB RAM, BlockManagerId(driver, bigdl.europe-west4-a.c.massive-network-289210.internal, 38787, None)
2020-10-23 10:10:41 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, bigdl.europe-west4-a.c.massive-network-289210.internal, 38787, None)
2020-10-23 10:10:41 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, bigdl.europe-west4-a.c.massive-network-289210.internal, 38787, None)
2020-10-23 10:10:41 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@505fca6f{/metrics/json,null,AVAILABLE,@Spark}
2020-10-23 10:10:43 INFO  CoarseGrainedSchedulerBackend$DriverEndpoint:54 - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.164.0.3:54920) with ID 0
2020-10-23 10:10:43 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 10.164.0.3:36551 with 366.3 MB RAM, BlockManagerId(0, 10.164.0.3, 36551, None)
2020-10-23 10:10:43 INFO  CoarseGrainedSchedulerBackend$DriverEndpoint:54 - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.164.0.4:34566) with ID 1
2020-10-23 10:10:43 INFO  StandaloneSchedulerBackend:54 - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 1.0
2020-10-23 10:10:43 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 10.164.0.4:41747 with 366.3 MB RAM, BlockManagerId(1, 10.164.0.4, 41747, None)
2020-10-23 10:10:44 INFO  Engine$:121 - Auto detect executor number and executor cores number
2020-10-23 10:10:44 INFO  Engine$:123 - Executor number is 2 and executor cores number is 2
2020-10-23 10:10:46 INFO  ThreadPool$:95 - Set mkl threads to 1 on thread 17
2020-10-23 10:10:46 INFO  Engine$:446 - Find existing spark context. Checking the spark conf...
cls.getname: com.intel.analytics.bigdl.python.api.Sample
BigDLBasePickler registering: bigdl.util.common  Sample
cls.getname: com.intel.analytics.bigdl.python.api.EvaluatedResult
BigDLBasePickler registering: bigdl.util.common  EvaluatedResult
cls.getname: com.intel.analytics.bigdl.python.api.JTensor
BigDLBasePickler registering: bigdl.util.common  JTensor
cls.getname: com.intel.analytics.bigdl.python.api.JActivity
BigDLBasePickler registering: bigdl.util.common  JActivity
0.01
('Extracting', '/home/am72ghiassi/bd/quantitative_performance_evaluation_group_6/data/experiments/repetition_5/le_20_in_20/train-images-idx3-ubyte.gz')
('Extracting', '/home/am72ghiassi/bd/quantitative_performance_evaluation_group_6/data/experiments/repetition_5/le_20_in_20/train-labels-idx1-ubyte.gz')
('Extracting', '/home/am72ghiassi/bd/quantitative_performance_evaluation_group_6/data/experiments/repetition_5/le_20_in_20/t10k-images-idx3-ubyte.gz')
('Extracting', '/home/am72ghiassi/bd/quantitative_performance_evaluation_group_6/data/experiments/repetition_5/le_20_in_20/t10k-labels-idx1-ubyte.gz')
creating: createSequential
creating: createReshape
creating: createSpatialConvolution
creating: createTanh
creating: createSpatialMaxPooling
creating: createSpatialConvolution
creating: createTanh
creating: createSpatialMaxPooling
creating: createReshape
creating: createLinear
creating: createTanh
creating: createLinear
creating: createLogSoftMax
creating: createClassNLLCriterion
creating: createDefault
creating: createSGD
creating: createMaxEpoch
creating: createDistriOptimizer
disableCheckSingleton is deprecated. Please use bigdl.check.singleton instead
creating: createEveryEpoch
creating: createTop1Accuracy
creating: createEveryEpoch
creating: createTrainSummary
creating: createValidationSummary
('saving logs to ', 'lenet520201023-101048')
2020-10-23 10:10:48 INFO  DistriOptimizer$:808 - caching training rdd ...
2020-10-23 10:11:02 ERROR TaskSetManager:70 - Task 2 in stage 0.0 failed 4 times; aborting job
Traceback (most recent call last):
  File "/home/am72ghiassi/bd/quantitative_performance_evaluation_group_6/lenet5.py", line 94, in <module>
    trained_model = optimizer.optimize()
  File "/home/am72ghiassi/bd/spark/lib/bigdl-0.11.0-python-api.zip/bigdl/optim/optimizer.py", line 790, in optimize
  File "/home/am72ghiassi/bd/spark/lib/bigdl-0.11.0-python-api.zip/bigdl/util/common.py", line 645, in callJavaFunc
  File "/home/am72ghiassi/bd/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
  File "/home/am72ghiassi/bd/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o109.optimize.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 0.0 failed 4 times, most recent failure: Lost task 2.3 in stage 0.0 (TID 8, 10.164.0.4, executor 1): java.lang.NoClassDefFoundError: Could not initialize class com.intel.analytics.bigdl.tensor.Tensor$
	at com.intel.analytics.bigdl.python.api.PythonBigDL.toTensor(PythonBigDL.scala:152)
	at com.intel.analytics.bigdl.python.api.PythonBigDL$$anonfun$toJSample$2.apply(PythonBigDL.scala:221)
	at com.intel.analytics.bigdl.python.api.PythonBigDL$$anonfun$toJSample$2.apply(PythonBigDL.scala:221)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at com.intel.analytics.bigdl.python.api.PythonBigDL.toJSample(PythonBigDL.scala:221)
	at com.intel.analytics.bigdl.python.api.PythonBigDL$$anonfun$toJSample$4.apply(PythonBigDL.scala:226)
	at com.intel.analytics.bigdl.python.api.PythonBigDL$$anonfun$toJSample$4.apply(PythonBigDL.scala:226)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)
	at org.apache.spark.rdd.RDD.count(RDD.scala:1162)
	at com.intel.analytics.bigdl.dataset.DistributedDataSet$$anon$5.cache(DataSet.scala:195)
	at com.intel.analytics.bigdl.optim.AbstractOptimizer.prepareInput(AbstractOptimizer.scala:281)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.prepareInput(DistriOptimizer.scala:809)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:869)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NoClassDefFoundError: Could not initialize class com.intel.analytics.bigdl.tensor.Tensor$
	at com.intel.analytics.bigdl.python.api.PythonBigDL.toTensor(PythonBigDL.scala:152)
	at com.intel.analytics.bigdl.python.api.PythonBigDL$$anonfun$toJSample$2.apply(PythonBigDL.scala:221)
	at com.intel.analytics.bigdl.python.api.PythonBigDL$$anonfun$toJSample$2.apply(PythonBigDL.scala:221)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at com.intel.analytics.bigdl.python.api.PythonBigDL.toJSample(PythonBigDL.scala:221)
	at com.intel.analytics.bigdl.python.api.PythonBigDL$$anonfun$toJSample$4.apply(PythonBigDL.scala:226)
	at com.intel.analytics.bigdl.python.api.PythonBigDL$$anonfun$toJSample$4.apply(PythonBigDL.scala:226)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2020-10-23 10:11:02 ERROR TransportRequestHandler:210 - Error while invoking RpcHandler#receive() for one-way message.
org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.
	at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160)
	at org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655)
	at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:208)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:113)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
